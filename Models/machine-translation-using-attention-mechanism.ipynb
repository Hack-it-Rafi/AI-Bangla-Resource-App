{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:37:41.221542Z",
     "iopub.status.busy": "2024-03-20T15:37:41.220764Z",
     "iopub.status.idle": "2024-03-20T15:38:03.048359Z",
     "shell.execute_reply": "2024-03-20T15:38:03.047579Z",
     "shell.execute_reply.started": "2024-03-20T15:37:41.221508Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 15:37:46.392416: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-20 15:37:46.392502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-20 15:37:46.724547: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Embedding, LSTM, Concatenate, Dropout, TimeDistributed,\n",
    "                                     Input, Dense, Bidirectional, Layer)\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import Progbar\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T16:53:49.210611Z",
     "iopub.status.busy": "2024-03-20T16:53:49.209886Z",
     "iopub.status.idle": "2024-03-20T16:54:00.906523Z",
     "shell.execute_reply": "2024-03-20T16:54:00.905348Z",
     "shell.execute_reply.started": "2024-03-20T16:53:49.210572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.15.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: explainable-ai-sdk, tensorflow-cloud, tensorflow-decision-forests, tensorflow-serving-api, tensorflow-text, witwidget\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:38:27.552495Z",
     "iopub.status.busy": "2024-03-20T15:38:27.551625Z",
     "iopub.status.idle": "2024-03-20T15:38:27.846017Z",
     "shell.execute_reply": "2024-03-20T15:38:27.845200Z",
     "shell.execute_reply.started": "2024-03-20T15:38:27.552458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('/kaggle/input/english-to-bengali-for-machine-translation/english to bengali.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:38:28.726867Z",
     "iopub.status.busy": "2024-03-20T15:38:28.726275Z",
     "iopub.status.idle": "2024-03-20T15:38:28.768148Z",
     "shell.execute_reply": "2024-03-20T15:38:28.767349Z",
     "shell.execute_reply.started": "2024-03-20T15:38:28.726837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_table('/kaggle/input/bilingual-sentence-pairs/ben.txt', header=None)\n",
    "data.columns = ['english_caption','bengali_caption','license']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:58:20.751633Z",
     "iopub.status.busy": "2024-03-19T14:58:20.751252Z",
     "iopub.status.idle": "2024-03-19T14:58:20.834673Z",
     "shell.execute_reply": "2024-03-19T14:58:20.833608Z",
     "shell.execute_reply.started": "2024-03-19T14:58:20.751605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Specify the file path\n",
    "file_path = '/combined_data.txt'\n",
    "\n",
    "# Open the file and read line by line, limiting to the first 50000 lines\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line_number, line in enumerate(file):\n",
    "        if line_number >= 20000:\n",
    "            break  # Break loop after reading 50000 lines\n",
    "        # Split each line based on the tab delimiter\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "        # Take only the first column (assuming the desired data is in the first column)\n",
    "        first_column = parts[0]\n",
    "        \n",
    "        # Append the first column to the data list\n",
    "        data.append([first_column])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "ban = pd.DataFrame(data, columns=['bengali_caption'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:58:22.863995Z",
     "iopub.status.busy": "2024-03-19T14:58:22.862871Z",
     "iopub.status.idle": "2024-03-19T14:58:22.879480Z",
     "shell.execute_reply": "2024-03-19T14:58:22.878495Z",
     "shell.execute_reply.started": "2024-03-19T14:58:22.863961Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bengali_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>এ জন্য সৎ, মেধাবী ও ত্যাগীদের বাছাই করে নেতৃত্...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>আমার মা,থমকে গেল এক মুহূর্তের মধ্যে কি হয়ে গেল।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>বিপুল প্রতিদন্দ্বিতার মধ্যে ৩ কেন্দ্রে ৪,৫ ও ৬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ওজিওয়াটস অ্যাপ্লিকেশন আপনাকে দুটি পৃথক ফোন নম...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>অনুষ্ঠানে সভাপতিত্ব করবেন আবৃত্তিশিল্পী ও স্বা...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     bengali_caption\n",
       "0  এ জন্য সৎ, মেধাবী ও ত্যাগীদের বাছাই করে নেতৃত্...\n",
       "1   আমার মা,থমকে গেল এক মুহূর্তের মধ্যে কি হয়ে গেল।\n",
       "2  বিপুল প্রতিদন্দ্বিতার মধ্যে ৩ কেন্দ্রে ৪,৫ ও ৬...\n",
       "3  ওজিওয়াটস অ্যাপ্লিকেশন আপনাকে দুটি পৃথক ফোন নম...\n",
       "4  অনুষ্ঠানে সভাপতিত্ব করবেন আবৃত্তিশিল্পী ও স্বা..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ban.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:58:29.969117Z",
     "iopub.status.busy": "2024-03-19T14:58:29.968398Z",
     "iopub.status.idle": "2024-03-19T14:58:30.024464Z",
     "shell.execute_reply": "2024-03-19T14:58:30.023411Z",
     "shell.execute_reply.started": "2024-03-19T14:58:29.969088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store data\n",
    "data = []\n",
    "\n",
    "# Specify the file path\n",
    "file_path = '/kaggle/input/english-to-bengali-dataset/1_Eng.txt'\n",
    "\n",
    "# Open the file and read line by line, limiting to the first 50000 lines\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line_number, line in enumerate(file):\n",
    "        if line_number >= 20000:\n",
    "            break  # Break loop after reading 50000 lines\n",
    "        # Split each line based on the tab delimiter\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "        # Take only the first column (assuming the desired data is in the first column)\n",
    "        first_column = parts[0]\n",
    "        \n",
    "        # Append the first column to the data list\n",
    "        data.append([first_column])\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "eng = pd.DataFrame(data, columns=['english_caption'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T16:43:28.908724Z",
     "iopub.status.busy": "2024-03-19T16:43:28.907940Z",
     "iopub.status.idle": "2024-03-19T16:43:29.637241Z",
     "shell.execute_reply": "2024-03-19T16:43:29.636006Z",
     "shell.execute_reply.started": "2024-03-19T16:43:28.908687Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meng\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eng' is not defined"
     ]
    }
   ],
   "source": [
    "eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T13:58:12.361138Z",
     "iopub.status.busy": "2024-03-19T13:58:12.360767Z",
     "iopub.status.idle": "2024-03-19T13:58:12.371141Z",
     "shell.execute_reply": "2024-03-19T13:58:12.370077Z",
     "shell.execute_reply.started": "2024-03-19T13:58:12.361111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_caption</th>\n",
       "      <th>bengali_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a child in a pink dress is climbing up a set o...</td>\n",
       "      <td>একটি গোলাপী জামা পরা বাচ্চা মেয়ে একটি বাড়ির প্...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a girl going into a wooden building .</td>\n",
       "      <td>একটি মেয়ে শিশু একটি কাঠের বাড়িতে ঢুকছে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a little girl climbing into a wooden playhouse .</td>\n",
       "      <td>একটি বাচ্চা তার কাঠের খেলাঘরে উঠছে ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a little girl climbing the stairs to her playh...</td>\n",
       "      <td>ছোট মেয়েটি তার খেলার ঘরের সিড়ি বেয়ে উঠছে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a little girl in a pink dress going into a woo...</td>\n",
       "      <td>গোলাপি জামা পড়া ছোট একটি মেয়ে একটি কাঠের তৈরি...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     english_caption  \\\n",
       "0  a child in a pink dress is climbing up a set o...   \n",
       "1              a girl going into a wooden building .   \n",
       "2   a little girl climbing into a wooden playhouse .   \n",
       "3  a little girl climbing the stairs to her playh...   \n",
       "4  a little girl in a pink dress going into a woo...   \n",
       "\n",
       "                                     bengali_caption  \n",
       "0  একটি গোলাপী জামা পরা বাচ্চা মেয়ে একটি বাড়ির প্...  \n",
       "1             একটি মেয়ে শিশু একটি কাঠের বাড়িতে ঢুকছে  \n",
       "2               একটি বাচ্চা তার কাঠের খেলাঘরে উঠছে ।  \n",
       "3           ছোট মেয়েটি তার খেলার ঘরের সিড়ি বেয়ে উঠছে  \n",
       "4  গোলাপি জামা পড়া ছোট একটি মেয়ে একটি কাঠের তৈরি...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:58:36.482319Z",
     "iopub.status.busy": "2024-03-19T14:58:36.481955Z",
     "iopub.status.idle": "2024-03-19T14:58:36.489046Z",
     "shell.execute_reply": "2024-03-19T14:58:36.488003Z",
     "shell.execute_reply.started": "2024-03-19T14:58:36.482294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat([eng, ban], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:58:38.159228Z",
     "iopub.status.busy": "2024-03-19T14:58:38.158594Z",
     "iopub.status.idle": "2024-03-19T14:58:38.169094Z",
     "shell.execute_reply": "2024-03-19T14:58:38.168078Z",
     "shell.execute_reply.started": "2024-03-19T14:58:38.159195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_caption</th>\n",
       "      <th>bengali_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For this, some of the party's middle-rung lead...</td>\n",
       "      <td>এ জন্য সৎ, মেধাবী ও ত্যাগীদের বাছাই করে নেতৃত্...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mother stopped what had happened in a moment.</td>\n",
       "      <td>আমার মা,থমকে গেল এক মুহূর্তের মধ্যে কি হয়ে গেল।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mim Chowdhury (Helicopter), daughter of the la...</td>\n",
       "      <td>বিপুল প্রতিদন্দ্বিতার মধ্যে ৩ কেন্দ্রে ৪,৫ ও ৬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The OGWats app allows you to use two separate ...</td>\n",
       "      <td>ওজিওয়াটস অ্যাপ্লিকেশন আপনাকে দুটি পৃথক ফোন নম...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ashraful Alam, a recitation artist and the voi...</td>\n",
       "      <td>অনুষ্ঠানে সভাপতিত্ব করবেন আবৃত্তিশিল্পী ও স্বা...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     english_caption  \\\n",
       "0  For this, some of the party's middle-rung lead...   \n",
       "1   My mother stopped what had happened in a moment.   \n",
       "2  Mim Chowdhury (Helicopter), daughter of the la...   \n",
       "3  The OGWats app allows you to use two separate ...   \n",
       "4  Ashraful Alam, a recitation artist and the voi...   \n",
       "\n",
       "                                     bengali_caption  \n",
       "0  এ জন্য সৎ, মেধাবী ও ত্যাগীদের বাছাই করে নেতৃত্...  \n",
       "1   আমার মা,থমকে গেল এক মুহূর্তের মধ্যে কি হয়ে গেল।  \n",
       "2  বিপুল প্রতিদন্দ্বিতার মধ্যে ৩ কেন্দ্রে ৪,৫ ও ৬...  \n",
       "3  ওজিওয়াটস অ্যাপ্লিকেশন আপনাকে দুটি পৃথক ফোন নম...  \n",
       "4  অনুষ্ঠানে সভাপতিত্ব করবেন আবৃত্তিশিল্পী ও স্বা...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:58:40.891149Z",
     "iopub.status.busy": "2024-03-19T14:58:40.890445Z",
     "iopub.status.idle": "2024-03-19T14:58:40.898313Z",
     "shell.execute_reply": "2024-03-19T14:58:40.896997Z",
     "shell.execute_reply.started": "2024-03-19T14:58:40.891116Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My mother stopped what had happened in a moment.\n",
      "\n",
      "\n",
      "আমার মা,থমকে গেল এক মুহূর্তের মধ্যে কি হয়ে গেল।\n"
     ]
    }
   ],
   "source": [
    "print(combined_df['english_caption'][1])\n",
    "print('\\n')\n",
    "print(combined_df['bengali_caption'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:58:53.885487Z",
     "iopub.status.busy": "2024-03-19T14:58:53.884883Z",
     "iopub.status.idle": "2024-03-19T14:58:53.895713Z",
     "shell.execute_reply": "2024-03-19T14:58:53.894774Z",
     "shell.execute_reply.started": "2024-03-19T14:58:53.885457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_caption</th>\n",
       "      <th>bengali_caption</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যাও।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যান।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যা।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালাও!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালান!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english_caption bengali_caption  \\\n",
       "0             Go.            যাও।   \n",
       "1             Go.            যান।   \n",
       "2             Go.             যা।   \n",
       "3            Run!          পালাও!   \n",
       "4            Run!          পালান!   \n",
       "\n",
       "                                             license  \n",
       "0  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "1  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "2  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "3  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
       "4  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:38:39.356535Z",
     "iopub.status.busy": "2024-03-20T15:38:39.356144Z",
     "iopub.status.idle": "2024-03-20T15:38:39.372533Z",
     "shell.execute_reply": "2024-03-20T15:38:39.371658Z",
     "shell.execute_reply.started": "2024-03-20T15:38:39.356507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.drop(['license'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:38:42.769005Z",
     "iopub.status.busy": "2024-03-20T15:38:42.768655Z",
     "iopub.status.idle": "2024-03-20T15:38:42.775962Z",
     "shell.execute_reply": "2024-03-20T15:38:42.775061Z",
     "shell.execute_reply.started": "2024-03-20T15:38:42.768972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.concat([data, df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T10:21:27.886805Z",
     "iopub.status.busy": "2024-03-20T10:21:27.886411Z",
     "iopub.status.idle": "2024-03-20T10:21:27.902975Z",
     "shell.execute_reply": "2024-03-20T10:21:27.901944Z",
     "shell.execute_reply.started": "2024-03-20T10:21:27.886776Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_caption</th>\n",
       "      <th>bengali_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যাও।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যান।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যা।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালাও!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালান!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english_caption bengali_caption\n",
       "0             Go.            যাও।\n",
       "1             Go.            যান।\n",
       "2             Go.             যা।\n",
       "3            Run!          পালাও!\n",
       "4            Run!          পালান!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T16:46:08.648642Z",
     "iopub.status.busy": "2024-03-19T16:46:08.647672Z",
     "iopub.status.idle": "2024-03-19T16:46:08.686469Z",
     "shell.execute_reply": "2024-03-19T16:46:08.684905Z",
     "shell.execute_reply.started": "2024-03-19T16:46:08.648604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mdf1\u001b[49m, combined_df], ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "final_df = pd.concat([df1, combined_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:59:01.190279Z",
     "iopub.status.busy": "2024-03-19T14:59:01.189939Z",
     "iopub.status.idle": "2024-03-19T14:59:01.199678Z",
     "shell.execute_reply": "2024-03-19T14:59:01.198672Z",
     "shell.execute_reply.started": "2024-03-19T14:59:01.190254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_caption</th>\n",
       "      <th>bengali_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যাও।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যান।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যা।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালাও!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালান!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english_caption bengali_caption\n",
       "0             Go.            যাও।\n",
       "1             Go.            যান।\n",
       "2             Go.             যা।\n",
       "3            Run!          পালাও!\n",
       "4            Run!          পালান!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T14:59:02.827553Z",
     "iopub.status.busy": "2024-03-19T14:59:02.827175Z",
     "iopub.status.idle": "2024-03-19T14:59:02.926129Z",
     "shell.execute_reply": "2024-03-19T14:59:02.924794Z",
     "shell.execute_reply.started": "2024-03-19T14:59:02.827512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_caption    0\n",
      "bengali_caption    0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(final_df.isnull().sum())\n",
    "print(final_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function To Preprocess English Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:38:48.485531Z",
     "iopub.status.busy": "2024-03-20T15:38:48.485147Z",
     "iopub.status.idle": "2024-03-20T15:38:48.491362Z",
     "shell.execute_reply": "2024-03-20T15:38:48.490280Z",
     "shell.execute_reply.started": "2024-03-20T15:38:48.485501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "remove_digits = str.maketrans('', '', string.digits) # Set of all digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:38:50.954555Z",
     "iopub.status.busy": "2024-03-20T15:38:50.953684Z",
     "iopub.status.idle": "2024-03-20T15:38:50.961298Z",
     "shell.execute_reply": "2024-03-20T15:38:50.960256Z",
     "shell.execute_reply.started": "2024-03-20T15:38:50.954516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_eng_sentence(sent):\n",
    "    '''Function to preprocess English Sentence'''\n",
    "    sent = sent.lower()\n",
    "    sent = sent.replace(\"'\", '')  # Changed .sub() to .replace()\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.translate(remove_digits)\n",
    "    \n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:38:56.362105Z",
     "iopub.status.busy": "2024-03-20T15:38:56.361708Z",
     "iopub.status.idle": "2024-03-20T15:38:56.368783Z",
     "shell.execute_reply": "2024-03-20T15:38:56.367711Z",
     "shell.execute_reply.started": "2024-03-20T15:38:56.362072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_ban_sentence(sent):\n",
    "    '''Function to preprocess Bengali Sentence'''  # Corrected function description\n",
    "    sent = sent.replace(\"'\", '')  # Changed .sub() to .replace()\n",
    "    sent = sent.replace(\"।\", '')\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.translate(remove_digits)\n",
    "    \n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    sent = \"startseq \" + sent + \" endseq\"  # Added space before <end>\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Pairs of Clean English and Bangla Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:11.186409Z",
     "iopub.status.busy": "2024-03-20T15:39:11.186035Z",
     "iopub.status.idle": "2024-03-20T15:39:12.865553Z",
     "shell.execute_reply": "2024-03-20T15:39:12.864741Z",
     "shell.execute_reply.started": "2024-03-20T15:39:11.186378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df1['english_caption'] = df1['english_caption'].apply(preprocess_eng_sentence)\n",
    "df1['bengali_caption'] = df1['bengali_caption'].apply(preprocess_ban_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:15.650470Z",
     "iopub.status.busy": "2024-03-20T15:39:15.649764Z",
     "iopub.status.idle": "2024-03-20T15:39:15.668883Z",
     "shell.execute_reply": "2024-03-20T15:39:15.667946Z",
     "shell.execute_reply.started": "2024-03-20T15:39:15.650436Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_caption</th>\n",
       "      <th>bengali_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>startseq যাও endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>startseq যান endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go</td>\n",
       "      <td>startseq যা endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>startseq পালাও endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run</td>\n",
       "      <td>startseq পালান endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english_caption        bengali_caption\n",
       "0              go    startseq যাও endseq\n",
       "1              go    startseq যান endseq\n",
       "2              go     startseq যা endseq\n",
       "3             run  startseq পালাও endseq\n",
       "4             run  startseq পালান endseq"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:17.269831Z",
     "iopub.status.busy": "2024-03-20T15:39:17.268939Z",
     "iopub.status.idle": "2024-03-20T15:39:17.274069Z",
     "shell.execute_reply": "2024-03-20T15:39:17.273062Z",
     "shell.execute_reply.started": "2024-03-20T15:39:17.269793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Vector Transformation\n",
    "\n",
    "* First we will build the vocabulary for both the source and target sentences\n",
    "* We will use this vocabulary to index each word in the sentence and convert it into it's numeric vector representation\n",
    "* Vocabulary should only be build on the training sentences and not the test sentences, as test sentences might contain some new words. These new words are tyically represented as out vocabulary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:19.083139Z",
     "iopub.status.busy": "2024-03-20T15:39:19.082203Z",
     "iopub.status.idle": "2024-03-20T15:39:19.089264Z",
     "shell.execute_reply": "2024-03-20T15:39:19.088374Z",
     "shell.execute_reply.started": "2024-03-20T15:39:19.083108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_sentences = df1['english_caption'].tolist()\n",
    "ban_sentences = df1['bengali_caption'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:20.668149Z",
     "iopub.status.busy": "2024-03-20T15:39:20.667264Z",
     "iopub.status.idle": "2024-03-20T15:39:20.675338Z",
     "shell.execute_reply": "2024-03-20T15:39:20.674201Z",
     "shell.execute_reply.started": "2024-03-20T15:39:20.668115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n = len(eng_sentences)\n",
    "split_index1 = int(0.95 * n)\n",
    "split_index2 = int(0.98 * n)\n",
    "\n",
    "train_eng_sents = eng_sentences[:split_index2]\n",
    "test_eng_sents = eng_sentences[split_index2:]\n",
    "\n",
    "train_ban_sents = ban_sentences[:split_index2]\n",
    "test_ban_sents = ban_sentences[split_index2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:21.720280Z",
     "iopub.status.busy": "2024-03-20T15:39:21.719845Z",
     "iopub.status.idle": "2024-03-20T15:39:21.727718Z",
     "shell.execute_reply": "2024-03-20T15:39:21.726425Z",
     "shell.execute_reply.started": "2024-03-20T15:39:21.720244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42529, 868)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_eng_sents), len(test_eng_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:24.256787Z",
     "iopub.status.busy": "2024-03-20T15:39:24.256384Z",
     "iopub.status.idle": "2024-03-20T15:39:26.146234Z",
     "shell.execute_reply": "2024-03-20T15:39:26.144818Z",
     "shell.execute_reply.started": "2024-03-20T15:39:24.256758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size: 9372\n",
      "Bangla Vocab Size: 17443\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(oov_token = '<OOV>')\n",
    "eng_tokenizer.fit_on_texts(train_eng_sents)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) +1\n",
    "\n",
    "ban_tokenizer = Tokenizer()\n",
    "ban_tokenizer.fit_on_texts(train_ban_sents)\n",
    "ban_vocab_size = len(ban_tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"English Vocab Size: {eng_vocab_size}\\nBangla Vocab Size: {ban_vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "* Since all the sentences wil not be of the same length, we will pad them to be of the maximum length that we decided earlier\n",
    "\n",
    "* Once the text vectorization step is done we will split them into their input and output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:27.665523Z",
     "iopub.status.busy": "2024-03-20T15:39:27.665062Z",
     "iopub.status.idle": "2024-03-20T15:39:29.629605Z",
     "shell.execute_reply": "2024-03-20T15:39:29.628305Z",
     "shell.execute_reply.started": "2024-03-20T15:39:27.665485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_sequences = eng_tokenizer.texts_to_sequences(train_eng_sents)\n",
    "ban_sequences = ban_tokenizer.texts_to_sequences(train_ban_sents)\n",
    "\n",
    "source_seqs = pad_sequences(eng_sequences, maxlen = max_length, padding = 'post')\n",
    "target_seqs = pad_sequences(ban_sequences, maxlen = max_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:29.632426Z",
     "iopub.status.busy": "2024-03-20T15:39:29.631965Z",
     "iopub.status.idle": "2024-03-20T15:39:30.769947Z",
     "shell.execute_reply": "2024-03-20T15:39:30.768825Z",
     "shell.execute_reply.started": "2024-03-20T15:39:29.632386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((source_seqs[:split_index1], target_seqs[:split_index1]))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=split_index1).batch(32, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((source_seqs[split_index1:], target_seqs[split_index1:]))\n",
    "val_dataset = val_dataset.batch(32, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custome Loss Function\n",
    "\n",
    "* we will use a custome lss funtion where we will mask out whenever os are present in the output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:34.829612Z",
     "iopub.status.busy": "2024-03-20T15:39:34.829229Z",
     "iopub.status.idle": "2024-03-20T15:39:34.836171Z",
     "shell.execute_reply": "2024-03-20T15:39:34.834899Z",
     "shell.execute_reply.started": "2024-03-20T15:39:34.829579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_obj(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "Attention mechanisms are a type of neural network layer that can be added to deep learing models. They allow the model to focus on specific parts of input  by assigning different weights to different weights to different parts of the input. This weighting is typically based on the relevence of each part of the input to the task at hand. Similary for the task of Neural Machine  Translation, the target sentence word will be alligned with each of the source sentence words to identify it's relevance with all the words\n",
    "\n",
    "**Additive Attention -** This mechanism refers to Dzmitry Bahdanau's work title \"Neural Machine Translation by Jointly Learning to Align and Translate\". This, this tecnique is also known as Bahdanau attention. The additive attention is implemented as follows\n",
    "\n",
    "  * Addition of weeighted Decoder hidden state and Encoder hidden states\n",
    "  * Tanh function is applied over this addition\n",
    "  * Output of step 2 is passed through another linear layer\n",
    "  * Softmax function is applied to calculate the normalized allignment attention scores\n",
    "  * Attention scores are applied to the encoder hidden state to indentify the importance of each source word with respect to the next output word\n",
    "  \n",
    "**Multiplicative Attention -** This method is proposed by Thang Luong in the work title 'Effective Approaches to Attention-based Neural Machine Translation'. It is build on top of additive attention (a.k.a Bahdanau attention). there are three scoring functions that we can choose from - dot, general , concat,(we have used dot in our code)\n",
    "  * Matrix multiplication perfomed between Encoder hidden states and Deocder hidden state\n",
    "  * Softmax function is applied t calculate the normalized allignment attention scores\n",
    "  * Attention scores are applied to the Encoder hidden states to indentify the importance of each source word with respect to the next output word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:39.206442Z",
     "iopub.status.busy": "2024-03-20T15:39:39.205950Z",
     "iopub.status.idle": "2024-03-20T15:39:39.220092Z",
     "shell.execute_reply": "2024-03-20T15:39:39.218765Z",
     "shell.execute_reply.started": "2024-03-20T15:39:39.206395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, s_hidden, h_hidden):\n",
    "        s_hidden = tf.expand_dims(s_hidden, axis=1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(s_hidden) + self.W2(h_hidden)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * h_hidden\n",
    "        context_vector = tf.expand_dims(tf.reduce_sum(context_vector, axis=1),axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class LuongAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "    \n",
    "    def call(self, s_hidden, h_hidden):\n",
    "        score = tf.matmul(h_hidden, s_hidden, transpose_b=True)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * h_hidden\n",
    "        context_vector = tf.expand_dims(tf.reduce_sum(context_vector, axis=1),axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence2Sequence Model With Attention Layer\n",
    "\n",
    "## Encoder\n",
    "  * It uses RNN layer and converts the input words to corresponding hidden vectors. Each vector represents the current word and the contex of the word. The encoder takes the input sequence, one token at a time, and uses an to update it's hidden state, which summarizes the information inthe input sequence. The finel hidden state (state_h in the code) of the encoder is then passed as the context vector to the decoder. Since we are using Bidirectional LSTM as our RNN, we will get 2 hidden ecoder states (forward_state_h and backward_state_h) and 2 cell states (forward_state_c and bacword_state_c). We concatenate them into one hidden and cell state (state_h, and state-c in the code)\n",
    "  \n",
    "  \n",
    "## Decoder\n",
    "  * It takes as input the hidden vector generated by the encoder, it's own hidden states, and the current word to preduce the next hidden vector and finally predict the next word. In out case, since it is a Bi LSTM, we also take the cell sate along with the hidden state. **When using the attention layer we first calculate the attention context vector by applying attention mechanism on the decoder hidden state and the encoder hidden states, once the context vector is preduced it is concatenated wirh the current word embeding and passed to the decoder to predict the next word. The decoder useds this new context vector and an initial hidden state to generate the output sequence, one token at a time. At each time step, the decoder uses the current hidden state, the attention alligned context vector, and the previous output token to generate a probability distribution over the possible next tokens. The token with the highest probability is then chosen as the output, and the process continues until the end of the output sequence (end) is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:45.975542Z",
     "iopub.status.busy": "2024-03-20T15:39:45.974244Z",
     "iopub.status.idle": "2024-03-20T15:39:45.984618Z",
     "shell.execute_reply": "2024-03-20T15:39:45.983387Z",
     "shell.execute_reply.started": "2024-03-20T15:39:45.975490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dims, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dims, mask_zero=True)\n",
    "        self.lstm = Bidirectional(LSTM(units, return_sequences=True, return_state=True))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        enc_output, forward_h, forward_c, backward_h, backward_c = self.lstm(x)\n",
    "        state_h = Concatenate()([forward_h, backward_h])\n",
    "        state_c = Concatenate()([forward_c, backward_c])\n",
    "        \n",
    "        return enc_output, state_h, state_c\n",
    "    \n",
    "    def summary(self):\n",
    "        \n",
    "        x = Input(shape=(None,))\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:47.853496Z",
     "iopub.status.busy": "2024-03-20T15:39:47.852674Z",
     "iopub.status.idle": "2024-03-20T15:39:47.863796Z",
     "shell.execute_reply": "2024-03-20T15:39:47.862908Z",
     "shell.execute_reply.started": "2024-03-20T15:39:47.853461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dims, units, use_additive=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dims, mask_zero=True)\n",
    "        self.lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.attention = BahdanauAttention(units) if use_additive else LuongAttention(units)\n",
    "        self.fc = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "        \n",
    "    def call(self, x, enc_output, state_h, state_c):\n",
    "        x = self.embedding(x)\n",
    "        context_vector, attn_weights = self.attention(state_h, enc_output)\n",
    "        context_vector = Concatenate(axis=-1)([context_vector, x])\n",
    "        dec_output, dec_h, dec_c = self.lstm(context_vector, initial_state=[state_h, state_c])\n",
    "        output = self.fc(dec_output)\n",
    "        \n",
    "        return output, dec_h, dec_c, attn_weights\n",
    "    \n",
    "    def summary(self):\n",
    "        x = Input(shape=(None,))\n",
    "        enc_output = Input(shape=(None, self.units))\n",
    "        state_h = Input(shape=(self.units,))\n",
    "        state_c = Input(shape=(self.units,))\n",
    "        model = Model(inputs=[x, enc_output, state_h, state_c], outputs=self.call(x, enc_output, state_h, state_c))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:50.222656Z",
     "iopub.status.busy": "2024-03-20T15:39:50.222287Z",
     "iopub.status.idle": "2024-03-20T15:39:50.268978Z",
     "shell.execute_reply": "2024-03-20T15:39:50.268057Z",
     "shell.execute_reply.started": "2024-03-20T15:39:50.222629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(eng_vocab_size, 512, 256)\n",
    "decoder = Decoder(ban_vocab_size, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:51.782142Z",
     "iopub.status.busy": "2024-03-20T15:39:51.781030Z",
     "iopub.status.idle": "2024-03-20T15:39:51.787122Z",
     "shell.execute_reply": "2024-03-20T15:39:51.785781Z",
     "shell.execute_reply.started": "2024-03-20T15:39:51.782096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# decoder = Decoder(ban_vocab_size, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:53.549666Z",
     "iopub.status.busy": "2024-03-20T15:39:53.549299Z",
     "iopub.status.idle": "2024-03-20T15:39:53.561609Z",
     "shell.execute_reply": "2024-03-20T15:39:53.560554Z",
     "shell.execute_reply.started": "2024-03-20T15:39:53.549636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    loss_value = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_output, state_h, state_c  = encoder(x)\n",
    "        dec_input = tf.expand_dims(y[:, 0], 1)\n",
    "        \n",
    "        for i in range(1, y.shape[1]):\n",
    "            dec_output, state_h, state_c, attn_w = decoder(dec_input, encoder_output, state_h, state_c)\n",
    "            loss_value += loss_function(y[:, i], dec_output[:, 0, :])\n",
    "            dec_input = tf.expand_dims(y[:, i], 1)\n",
    "    \n",
    "    batch_loss = (loss_value / int(y.shape[1]))  \n",
    "    weights = encoder.trainable_variables + decoder.trainable_variables\n",
    "    grads = tape.gradient(loss_value, weights)\n",
    "    optimizer.apply_gradients(zip(grads, weights))\n",
    "    return batch_loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    loss_value = 0\n",
    "    encoder_output, state_h, state_c  = encoder(x)\n",
    "    dec_input = tf.expand_dims(y[:, 0], 1)\n",
    "        \n",
    "    for i in range(1, y.shape[1]):\n",
    "        dec_output, state_h, state_c, attn_w = decoder(dec_input, encoder_output, state_h, state_c)\n",
    "        loss_value += loss_function(y[:, i], dec_output[:, 0, :])\n",
    "        dec_input = tf.expand_dims(y[:, i], 1)\n",
    "    \n",
    "    batch_loss = (loss_value / int(y.shape[1]))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T15:39:55.691384Z",
     "iopub.status.busy": "2024-03-20T15:39:55.690669Z",
     "iopub.status.idle": "2024-03-20T16:18:06.127578Z",
     "shell.execute_reply": "2024-03-20T16:18:06.126699Z",
     "shell.execute_reply.started": "2024-03-20T15:39:55.691349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 180ms/step - train_loss: 1.3215 - val_loss: 1.2156\n",
      "Model loss improved from inf to 1.21557 Checkpoint Created: encoder.weights.h5, decoder.weights.h5\n",
      "Epoch:  2\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 174ms/step - train_loss: 0.9453 - val_loss: 1.0229\n",
      "Model loss improved from 1.21557 to 1.02285 Checkpoint Created: encoder.weights.h5, decoder.weights.h5\n",
      "Epoch:  3\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 174ms/step - train_loss: 0.7527 - val_loss: 0.9488\n",
      "Model loss improved from 1.02285 to 0.94878 Checkpoint Created: encoder.weights.h5, decoder.weights.h5\n",
      "Epoch:  4\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 174ms/step - train_loss: 0.6121 - val_loss: 0.9147\n",
      "Model loss improved from 0.94878 to 0.91471 Checkpoint Created: encoder.weights.h5, decoder.weights.h5\n",
      "Epoch:  5\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 174ms/step - train_loss: 0.4970 - val_loss: 0.9082\n",
      "Model loss improved from 0.91471 to 0.90821 Checkpoint Created: encoder.weights.h5, decoder.weights.h5\n",
      "Epoch:  6\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 174ms/step - train_loss: 0.4045 - val_loss: 0.9086\n",
      "Epoch:  7\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 174ms/step - train_loss: 0.3328 - val_loss: 0.9197\n",
      "Epoch:  8\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 174ms/step - train_loss: 0.2771 - val_loss: 0.9363\n",
      "Epoch:  9\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 174ms/step - train_loss: 0.2300 - val_loss: 0.9620\n",
      "Epoch:  10\n",
      "\u001b[1m1288/1288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 174ms/step - train_loss: 0.1902 - val_loss: 0.9786\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "epochs = 10\n",
    "train_steps = np.sum([1 for i in train_dataset])\n",
    "val_steps = np.sum([1 for i in val_dataset])  # Calculate number of validation steps\n",
    "metrics_names = ['train_loss', 'val_loss']  # Include 'val_loss' in metrics_names\n",
    "mon_val_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch+1)\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    progBar = Progbar(train_steps, stateful_metrics=metrics_names)\n",
    "    \n",
    "    # Training loop\n",
    "    for step, (X_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        train_loss = train_step(X_batch_train, y_batch_train)\n",
    "        training_loss.append(train_loss)\n",
    "        values = [('train_loss', train_loss)]\n",
    "        progBar.update(step, values=values)\n",
    "    \n",
    "    avg_train_loss = np.average(training_loss)\n",
    "    values = [('train_loss', avg_train_loss)]\n",
    "    progBar.update(step, values)\n",
    "    \n",
    "    # Validation loop\n",
    "    for step, (X_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "        val_loss = test_step(X_batch_val, y_batch_val)\n",
    "        validation_loss.append(val_loss)\n",
    "    \n",
    "    avg_val_loss = np.average(validation_loss)\n",
    "    values.append(('val_loss', avg_val_loss))\n",
    "    progBar.update(train_steps, values=values, finalize=True)\n",
    "    \n",
    "    # Check if validation loss improved\n",
    "    if avg_val_loss < mon_val_loss:\n",
    "        encoder.save_weights(\"encoder.weights.h5\")\n",
    "        decoder.save_weights(\"decoder.weights.h5\")\n",
    "        print(f\"Model loss improved from {mon_val_loss:.5f} to {avg_val_loss:.5f} Checkpoint Created: encoder.weights.h5, decoder.weights.h5\")\n",
    "        mon_val_loss = avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T16:20:39.486759Z",
     "iopub.status.busy": "2024-03-20T16:20:39.486361Z",
     "iopub.status.idle": "2024-03-20T16:20:39.714097Z",
     "shell.execute_reply": "2024-03-20T16:20:39.713086Z",
     "shell.execute_reply.started": "2024-03-20T16:20:39.486730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder.save_weights(\"encoder1.weights.h5\")\n",
    "decoder.save_weights(\"decoder1.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T16:57:19.222251Z",
     "iopub.status.busy": "2024-03-20T16:57:19.221890Z",
     "iopub.status.idle": "2024-03-20T16:57:19.395871Z",
     "shell.execute_reply": "2024-03-20T16:57:19.395017Z",
     "shell.execute_reply.started": "2024-03-20T16:57:19.222221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder.load_weights('/kaggle/working/encoder1.weights.h5')\n",
    "decoder.load_weights('/kaggle/working/decoder1.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T16:57:21.895626Z",
     "iopub.status.busy": "2024-03-20T16:57:21.894900Z",
     "iopub.status.idle": "2024-03-20T16:57:21.903313Z",
     "shell.execute_reply": "2024-03-20T16:57:21.902464Z",
     "shell.execute_reply.started": "2024-03-20T16:57:21.895592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_sentence(en_input):\n",
    "    eng_sequence = eng_tokenizer.texts_to_sequences([en_input])\n",
    "    en_input = pad_sequences(eng_sequence, maxlen=max_length, padding='post')\n",
    "    hidden_state, next_h, next_c = encoder(en_input)\n",
    "    attn_plot = []\n",
    "\n",
    "    curr_token = np.zeros((1,1))\n",
    "    curr_token[0,0] = ban_tokenizer.word_index['startseq']\n",
    "    pred_sentence = ''\n",
    "\n",
    "    for i in range(max_length):\n",
    "        output, next_h, next_c, attn_w = decoder(curr_token, hidden_state, next_h, next_c)\n",
    "        attn_plot.append(attn_w.numpy().reshape(-1,))\n",
    "        next_token = np.argmax(output[:, 0, :], axis=1)[0]\n",
    "        next_word = ban_tokenizer.index_word[next_token]\n",
    "        if next_word == 'endseq':\n",
    "            break\n",
    "        else:\n",
    "            pred_sentence += ' ' + next_word\n",
    "            curr_token[0,0] = next_token\n",
    "\n",
    "    return pred_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T16:57:23.095800Z",
     "iopub.status.busy": "2024-03-20T16:57:23.094965Z",
     "iopub.status.idle": "2024-03-20T16:57:23.441605Z",
     "shell.execute_reply": "2024-03-20T16:57:23.440773Z",
     "shell.execute_reply.started": "2024-03-20T16:57:23.095768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: আমার এখন খেতে ইচ্ছে করছে না\n"
     ]
    }
   ],
   "source": [
    "result = predict_sentence('i dont feel like eating anything now')\n",
    "print('Predicted translation: {}'.format(result))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T16:57:41.652722Z",
     "iopub.status.busy": "2024-03-20T16:57:41.652334Z",
     "iopub.status.idle": "2024-03-20T16:57:41.683100Z",
     "shell.execute_reply": "2024-03-20T16:57:41.682368Z",
     "shell.execute_reply.started": "2024-03-20T16:57:41.652693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('eng_tokenizer.pkl','wb') as file:\n",
    "    pickle.dump(eng_tokenizer, file)\n",
    "    \n",
    "import pickle\n",
    "with open('ban_tokenizer.pkl','wb') as file:\n",
    "    pickle.dump(ban_tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1118439,
     "sourceId": 1878727,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4389201,
     "sourceId": 7537383,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4485439,
     "sourceId": 7689472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
